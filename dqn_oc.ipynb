{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oydXZWrhKoBY"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import ale_py  # Ensure Atari environments work\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import collections\n",
        "import random\n",
        "from collections import deque\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "from functools import partial\n",
        "import os\n",
        "\n",
        "from utils import get_env, record_video_oc, get_obj_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-P3Fee-h1bey"
      },
      "outputs": [],
      "source": [
        "class DQN_OC(nn.Module):\n",
        "    def __init__(self, num_classes, hidden_dim, action_dim):\n",
        "        super(DQN_OC, self).__init__()\n",
        "\n",
        "        self.class_embs = nn.Embedding(num_embeddings=num_classes, embedding_dim=hidden_dim, padding_idx=0)\n",
        "        self.time_embs = nn.Embedding(num_embeddings=4, embedding_dim=hidden_dim)\n",
        "        self.xywh_proj = nn.Linear(4, hidden_dim)\n",
        "\n",
        "        self.encoder = nn.GRU(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=3,\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, action_dim)  # Output Q-values for each action\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        time_emb = self.time_embs(x[:, :, 0].long())\n",
        "        class_emb = self.class_embs(x[:, :, 1].long())\n",
        "        xywh_emb = self.xywh_proj(x[:, :, 2:])\n",
        "        obj_emb = time_emb + class_emb + xywh_emb\n",
        "        x, _ = self.encoder(obj_emb)\n",
        "        x, _ = torch.max(x, dim=1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xHgk5kkH-hBd"
      },
      "outputs": [],
      "source": [
        "def select_action(env, model, state_objs, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()  # Random action (exploration)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Add batch dimension\n",
        "        state_objs = state_objs.to_tensor()\n",
        "        state_objs = state_objs.unsqueeze(0).to(device)\n",
        "        return model(state_objs).squeeze().argmax().item()\n",
        "\n",
        "def train(model, target_model, buffer, optimizer, batch_size, gamma):\n",
        "    # print('train buffer.size():', buffer.size())\n",
        "    # print('batch_size:', batch_size)\n",
        "\n",
        "    if buffer.size() < batch_size:\n",
        "        return 0\n",
        "    \n",
        "    # Sample batch from experience replay\n",
        "    state_objs, actions, rewards, next_state_objs, dones = buffer.sample(batch_size)\n",
        "\n",
        "    state_objs = state_objs.to(device)\n",
        "    actions = actions.to(device)\n",
        "    rewards = rewards.to(device)\n",
        "    dones = dones.to(device)\n",
        "    next_state_objs = next_state_objs.to(device)\n",
        "\n",
        "    # Compute Q-values for current states\n",
        "    q = model(state_objs)\n",
        "    # print('q.shape:', q.shape)\n",
        "    q_values = q.gather(1, actions.unsqueeze(1)).squeeze(1)  # Select Q-values of taken actions\n",
        "\n",
        "    # Compute next Q-values from the target network\n",
        "    next_q_values = target_model(next_state_objs).max(1)[0].detach()  # Max Q-value of next state\n",
        "\n",
        "    dones = dones.to(torch.bool)\n",
        "    # Zero next_q_values for terminal states\n",
        "    next_q_values[dones] = 0.0\n",
        "\n",
        "    # Compute target Q-values\n",
        "    scaled_rewards = 0.01 * rewards\n",
        "    target_q_values = scaled_rewards + gamma * next_q_values\n",
        "\n",
        "    dq_loss = F.mse_loss(q_values, target_q_values.detach())\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    dq_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
        "    optimizer.step()\n",
        "    return dq_loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state_objs, action, reward, next_state_objs, done):\n",
        "        self.buffer.append((\n",
        "            state_objs,\n",
        "            action,\n",
        "            int(reward),\n",
        "            next_state_objs,\n",
        "            bool(done)\n",
        "        ))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state_objs, action, reward, next_state_objs, done = zip(*batch)\n",
        "\n",
        "        return (\n",
        "            pad_sequence((objs.to_tensor() for objs in state_objs), batch_first=True, padding_value=0.0),\n",
        "            torch.LongTensor(action),\n",
        "            torch.FloatTensor(reward),\n",
        "            pad_sequence((objs.to_tensor() for objs in next_state_objs), batch_first=True, padding_value=0.0),\n",
        "            torch.FloatTensor(done)\n",
        "        )\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1wuMoyn2Lc08",
        "outputId": "37786411-59ff-4c78-f703-249d8d91bd03"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
            "[Powered by Stella]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `wandb.require('core')` is redundant as it is now the default behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkevinxli\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20250316_033902-uovaexzh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kevinxli/frogger/runs/uovaexzh' target=\"_blank\">dqn_oc</a></strong> to <a href='https://wandb.ai/kevinxli/frogger' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kevinxli/frogger' target=\"_blank\">https://wandb.ai/kevinxli/frogger</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kevinxli/frogger/runs/uovaexzh' target=\"_blank\">https://wandb.ai/kevinxli/frogger/runs/uovaexzh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kevinxli/frogger/runs/uovaexzh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f384a285360>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the Atari environment\n",
        "env = get_env(process=False, oc=True)\n",
        "\n",
        "# Check Action / State space\n",
        "obs, info = env.reset()\n",
        "\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dqn = DQN_OC(num_classes=len(get_obj_classes())+1, hidden_dim=300, action_dim=action_dim).to(device)\n",
        "target_dqn = DQN_OC(num_classes=len(get_obj_classes())+1, hidden_dim=300, action_dim=action_dim).to(device)\n",
        "target_dqn.load_state_dict(dqn.state_dict())\n",
        "\n",
        "lr = 0.0001\n",
        "weight_decay = 1e-5\n",
        "replay_buffer_size = 10000\n",
        "optimizer = optim.AdamW(dqn.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "replay_buffer = ReplayBuffer(replay_buffer_size)\n",
        "\n",
        "num_train_iterations = 1000000\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.05\n",
        "epsilon_decay = 0.999885\n",
        "target_update_freq = 10000\n",
        "rewards_list = []\n",
        "\n",
        "project_name = 'dqn_oc'\n",
        "\n",
        "wandb.require(\"core\")\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "      # Set the project where this run will be logged\n",
        "      project=\"frogger\",\n",
        "      # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
        "      name=project_name,\n",
        "      # Track hyperparameters and run metadata\n",
        "      config={\n",
        "      \"lr\": lr,\n",
        "      \"weight_decay\": weight_decay,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"gamma\": gamma,\n",
        "      \"epsilon\": epsilon,\n",
        "      \"epsilon_min\": epsilon_min,\n",
        "      \"epsilon_decay\": epsilon_decay,\n",
        "      \"replay_buffer_size\": replay_buffer_size,\n",
        "      \"variant\": project_name,\n",
        "      \"num_train_iterations\": num_train_iterations,\n",
        "      \"target_update_freq\": target_update_freq,\n",
        "      })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000001 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1965/1000001 [00:21<2:56:45, 94.11it/s]"
          ]
        }
      ],
      "source": [
        "os.makedirs(f'{project_name}/train', exist_ok=True)\n",
        "\n",
        "optimizer = optim.AdamW(dqn.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "state, info = env.reset()\n",
        "total_loss = 0\n",
        "total_reward = 0\n",
        "\n",
        "for iteration in tqdm(range(num_train_iterations+1)):\n",
        "    action = select_action(env, dqn, state, epsilon)\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "    replay_buffer.push(state, action, reward, next_state, terminated)\n",
        "\n",
        "    if terminated or truncated:\n",
        "        state, info = env.reset()\n",
        "        wandb.log({'train/loss': total_loss})\n",
        "        total_loss = 0\n",
        "        total_reward = 0\n",
        "    else:\n",
        "        state = next_state\n",
        "\n",
        "    loss = train(dqn, target_dqn, replay_buffer, optimizer, batch_size, gamma)\n",
        "    total_loss += loss\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    rewards_list.append(total_reward)\n",
        "\n",
        "    if iteration % target_update_freq == 0:\n",
        "        target_dqn.load_state_dict(dqn.state_dict())\n",
        "        torch.save(dqn.state_dict(), f\"{project_name}/train/frogger_dqn_iter_{iteration}.pth\")\n",
        "        if iteration > 0:\n",
        "            reward, length, time = record_video_oc(select_action=partial(select_action, model=dqn, epsilon=0), video_folder=f\"{project_name}/videos\", video_name=f\"train_iter_{iteration}\")\n",
        "            wandb.log({'train/reward': reward, 'train/length': length, 'train/time': time})\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
