{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oydXZWrhKoBY"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import ale_py  # Ensure Atari environments work\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import collections\n",
        "import random\n",
        "from collections import deque\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "from functools import partial\n",
        "import os\n",
        "\n",
        "from utils import get_env, wrap_recording, load_demonstrations, record_video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-P3Fee-h1bey"
      },
      "outputs": [],
      "source": [
        "class DQN_CNN(nn.Module):\n",
        "    def __init__(self, input_channels, action_dim):\n",
        "        super(DQN_CNN, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),  # Output: (32, 20, 20)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),  # Output: (64, 9, 9)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),  # Output: (64, 7, 7)\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(64*7*7, 512),  # Flattened CNN features\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, action_dim)  # Output Q-values for each action\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xHgk5kkH-hBd"
      },
      "outputs": [],
      "source": [
        "def select_action(env, model, state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()  # Random action (exploration)\n",
        "\n",
        "    state = torch.FloatTensor(np.array(state)).unsqueeze(0) / 255.0  # Normalize pixels\n",
        "    state = state.to(device)\n",
        "    with torch.no_grad():\n",
        "        return model(state).argmax().item()\n",
        "\n",
        "def train(model, target_model, buffer, optimizer, batch_size, gamma, use_supervised_loss=False):\n",
        "    # Sample batch from experience replay\n",
        "    states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "\n",
        "    states = states.to(device)\n",
        "    actions = actions.to(device)\n",
        "    rewards = rewards.to(device)\n",
        "    next_states = next_states.to(device)\n",
        "    dones = dones.to(device)\n",
        "\n",
        "    # Compute Q-values for current states\n",
        "    q = model(states)\n",
        "    # print('q.shape:', q.shape)\n",
        "    q_values = q.gather(1, actions.unsqueeze(1)).squeeze(1)  # Select Q-values of taken actions\n",
        "\n",
        "    # Compute next Q-values from the target network\n",
        "    next_q_values = target_model(next_states).max(1)[0].detach()  # Max Q-value of next state\n",
        "\n",
        "    dones = dones.to(torch.bool)\n",
        "    # Zero next_q_values for terminal states\n",
        "    next_q_values[dones] = 0.0\n",
        "\n",
        "    # Compute target Q-values\n",
        "    scaled_rewards = 0.01 * rewards\n",
        "    target_q_values = scaled_rewards + gamma * next_q_values\n",
        "\n",
        "    dq_loss = F.mse_loss(q_values, target_q_values.detach())\n",
        "\n",
        "    if use_supervised_loss:\n",
        "        l = torch.full_like(q, 0.8)\n",
        "        l[:, actions] = 0\n",
        "        # print('actions', actions)\n",
        "        # print('l', l)\n",
        "        # print('q', q)\n",
        "        # print('q_values', q_values)\n",
        "        # print('q_values.shape:', q_values.shape)\n",
        "        supervised_loss = torch.mean((q + l).max(dim=-1)[0] - q_values)\n",
        "        loss = dq_loss + supervised_loss\n",
        "    else:\n",
        "        loss = dq_loss\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
        "    optimizer.step()\n",
        "    if use_supervised_loss:\n",
        "        return dq_loss.item(), supervised_loss.item()\n",
        "    else:\n",
        "        return loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, demonstrations):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "        self.demonstrations = demonstrations\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((\n",
        "            state,\n",
        "            action,\n",
        "            int(reward),\n",
        "            next_state,\n",
        "            bool(done)\n",
        "        ))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if len(self.buffer) < batch_size // 16:\n",
        "            batch = random.sample(self.demonstrations, batch_size)\n",
        "        else:\n",
        "            batch = random.sample(self.buffer, batch_size // 16)\n",
        "            batch += random.sample(self.demonstrations, batch_size // 16 * 15)\n",
        "            random.shuffle(batch)\n",
        "        state, action, reward, next_state, done = zip(*batch)\n",
        "\n",
        "        return (\n",
        "            torch.FloatTensor(np.array(state)) / 255.0,  # Normalize pixels\n",
        "            torch.LongTensor(action),\n",
        "            torch.FloatTensor(reward),\n",
        "            torch.FloatTensor(np.array(next_state)) / 255.0,\n",
        "            torch.FloatTensor(done)\n",
        "        )\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1wuMoyn2Lc08",
        "outputId": "37786411-59ff-4c78-f703-249d8d91bd03"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
            "[Powered by Stella]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation space: Box(0, 255, (84, 336), uint8)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `wandb.require('core')` is redundant as it is now the default behavior.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkevinxli\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20250311_062943-gazksxz7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kevinxli/frogger/runs/gazksxz7' target=\"_blank\">dqfd</a></strong> to <a href='https://wandb.ai/kevinxli/frogger' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kevinxli/frogger' target=\"_blank\">https://wandb.ai/kevinxli/frogger</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kevinxli/frogger/runs/gazksxz7' target=\"_blank\">https://wandb.ai/kevinxli/frogger/runs/gazksxz7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kevinxli/frogger/runs/gazksxz7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f3401768c70>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the Atari environment\n",
        "env = get_env()\n",
        "\n",
        "# Check Action / State space\n",
        "obs, info = env.reset()\n",
        "\n",
        "action_dim = env.action_space.n\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dqn = DQN_CNN(4, action_dim).to(device)\n",
        "target_dqn = DQN_CNN(4, action_dim).to(device)\n",
        "target_dqn.load_state_dict(dqn.state_dict())\n",
        "\n",
        "lr = 0.0001\n",
        "weight_decay = 1e-5\n",
        "replay_buffer_size = 10000\n",
        "optimizer = optim.AdamW(dqn.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "demonstrations = load_demonstrations(stack_frames=True)\n",
        "replay_buffer = ReplayBuffer(replay_buffer_size, demonstrations)\n",
        "\n",
        "num_pretraining_iterations = 100000\n",
        "num_train_iterations = 1000000\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "epsilon = 0.01\n",
        "target_update_freq = 10000\n",
        "rewards_list = []\n",
        "\n",
        "wandb.require(\"core\")\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "      # Set the project where this run will be logged\n",
        "      project=\"frogger\",\n",
        "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "      name=f\"dqfd\",\n",
        "      # Track hyperparameters and run metadata\n",
        "      config={\n",
        "      \"lr\": lr,\n",
        "      \"weight_decay\": weight_decay,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"gamma\": gamma,\n",
        "      \"epsilon\": epsilon,\n",
        "      \"replay_buffer_size\": replay_buffer_size,\n",
        "      \"variant\": \"dqfd\",\n",
        "      \"num_pretraining_iterations\": num_pretraining_iterations,\n",
        "      \"num_train_iterations\": num_train_iterations,\n",
        "      \"target_update_freq\": target_update_freq,\n",
        "      })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|▉         | 9985/100001 [00:53<06:43, 223.01it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /teamspace/studios/this_studio/dqfd/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "100%|██████████| 100001/100001 [07:59<00:00, 208.52it/s]\n"
          ]
        }
      ],
      "source": [
        "# Supervised pretraining\n",
        "\n",
        "os.makedirs('dqfd/pretrained', exist_ok=True)\n",
        "\n",
        "for iteration in tqdm(range(num_pretraining_iterations+1)):\n",
        "    dq_loss, s_loss = train(dqn, target_dqn, replay_buffer, optimizer, batch_size, gamma, use_supervised_loss=True)\n",
        "    wandb.log({\"pretrain/loss\": dq_loss + s_loss, \"pretrain/dq_loss\": dq_loss, \"pretrain/supervised_loss\": s_loss})\n",
        "    if iteration % target_update_freq == 0:\n",
        "        target_dqn.load_state_dict(dqn.state_dict())\n",
        "        # print(f\"DQ Loss {dq_loss:.5f} S Loss {s_loss:.5f}\")\n",
        "        reward, length, time = record_video(select_action=partial(select_action, model=dqn, epsilon=0), video_folder=\"dqfd/videos\", episode_trigger=lambda x: True, name_prefix=f\"pretrain_iter_{iteration}\")\n",
        "        # print(f'Episode total rewards: {reward}, lengths: {length}, time taken: {time}')\n",
        "        wandb.log({'pretrain/reward': reward, 'pretrain/length': length, 'pretrain/time': time})\n",
        "        torch.save(dqn.state_dict(), f'dqfd/pretrained/{iteration}.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dqn = DQN_CNN(4, action_dim).to(device)\n",
        "target_dqn = DQN_CNN(4, action_dim).to(device)\n",
        "dqn.load_state_dict(torch.load(f\"dqfd/pretrained/{num_pretraining_iterations-10000}.pth\", weights_only=True))\n",
        "target_dqn.load_state_dict(torch.load(f\"dqfd/pretrained/{num_pretraining_iterations}.pth\", weights_only=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /teamspace/studios/this_studio/dqfd/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "reward, length, time = record_video(select_action=partial(select_action, model=dqn, epsilon=0), video_folder=\"dqfd/videos\", episode_trigger=lambda x: True, name_prefix=f\"train_iter_0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /teamspace/studios/this_studio/dqfd/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m---> 23\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdqn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dqn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     26\u001b[0m rewards_list\u001b[38;5;241m.\u001b[39mappend(total_reward)\n",
            "Cell \u001b[0;32mIn[3], line 54\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, target_model, buffer, optimizer, batch_size, gamma, use_supervised_loss)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     53\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 54\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)  \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[1;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "os.makedirs('dqfd/train', exist_ok=True)\n",
        "\n",
        "optimizer = optim.AdamW(dqn.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "state, info = env.reset()\n",
        "total_loss = 0\n",
        "total_reward = 0\n",
        "\n",
        "for iteration in range(num_train_iterations+1):\n",
        "    action = select_action(env, dqn, state, epsilon)\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "    replay_buffer.push(state, action, reward, next_state, terminated)\n",
        "\n",
        "    if terminated:\n",
        "        state, info = env.reset()\n",
        "        wandb.log({'train/loss': total_loss})\n",
        "        total_loss = 0\n",
        "        total_reward = 0\n",
        "    else:\n",
        "        state = next_state\n",
        "\n",
        "    loss = train(dqn, target_dqn, replay_buffer, optimizer, batch_size, gamma)\n",
        "    total_loss += loss\n",
        "\n",
        "    rewards_list.append(total_reward)\n",
        "\n",
        "    if iteration % target_update_freq == 0:\n",
        "        target_dqn.load_state_dict(dqn.state_dict())\n",
        "        torch.save(dqn.state_dict(), f\"dqfd/train/frogger_dqfd_iter_{iteration}.pth\")\n",
        "        reward, length, time = record_video(select_action=partial(select_action, model=dqn, epsilon=0), video_folder=\"dqfd/videos\", episode_trigger=lambda x: True, name_prefix=f\"train_iter_{iteration}\")\n",
        "        wandb.log({'train/reward': reward, 'train/length': length, 'train/time': time})\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
