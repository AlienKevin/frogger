{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oydXZWrhKoBY"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import ale_py  # Ensure Atari environments work\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import collections\n",
        "import random\n",
        "from gymnasium.wrappers import GrayscaleObservation, ResizeObservation, RecordVideo, RecordEpisodeStatistics\n",
        "from collections import deque\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "from gymnasium import spaces\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "\n",
        "        See Also\n",
        "        --------\n",
        "        baselines.common.atari_wrappers.LazyFrames\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)), dtype=env.observation_space.dtype)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        ob, info = self.env.reset(seed=seed, options=options)\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob(), info\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, truncated, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, truncated, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\n",
        "\n",
        "        This object should only be converted to numpy array before being passed to the model.\n",
        "\n",
        "        You'd not believe how complex the previous solution was.\"\"\"\n",
        "        self._frames = frames\n",
        "        self._out = None\n",
        "\n",
        "    def _force(self):\n",
        "        if self._out is None:\n",
        "            self._out = np.stack(self._frames)\n",
        "            self._frames = None\n",
        "        return self._out\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = self._force()\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._force())\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._force()[i]\n",
        "\n",
        "    def count(self):\n",
        "        frames = self._force()\n",
        "        return frames.shape[frames.ndim - 1]\n",
        "\n",
        "    def frame(self, i):\n",
        "        return self._force()[..., i]\n",
        "\n",
        "\n",
        "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\n",
        "    \"\"\"Configure environment for DeepMind-style Atari.\n",
        "    \"\"\"\n",
        "    if episode_life:\n",
        "        env = EpisodicLifeEnv(env)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "    if scale:\n",
        "        env = ScaledFloatFrame(env)\n",
        "    if clip_rewards:\n",
        "        env = ClipRewardEnv(env)\n",
        "    if frame_stack:\n",
        "        env = FrameStack(env, 4)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-P3Fee-h1bey"
      },
      "outputs": [],
      "source": [
        "class DQN_CNN(nn.Module):\n",
        "    def __init__(self, input_channels, action_dim):\n",
        "        super(DQN_CNN, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),  # Output: (32, 20, 20)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),  # Output: (64, 9, 9)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),  # Output: (64, 7, 7)\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(64*7*7, 512),  # Flattened CNN features\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, action_dim)  # Output Q-values for each action\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xHgk5kkH-hBd"
      },
      "outputs": [],
      "source": [
        "def select_action(env, model, state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()  # Random action (exploration)\n",
        "\n",
        "    state = torch.FloatTensor(state).unsqueeze(0) / 255.0  # Normalize pixels\n",
        "    state = state.to(device)\n",
        "    with torch.no_grad():\n",
        "        return model(state).argmax().item()\n",
        "\n",
        "def train(model, target_model, buffer, optimizer, batch_size, gamma, use_supervised_loss=False):\n",
        "    # Sample batch from experience replay\n",
        "    states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "\n",
        "    states = states.to(device)\n",
        "    actions = actions.to(device)\n",
        "    rewards = rewards.to(device)\n",
        "    next_states = next_states.to(device)\n",
        "    dones = dones.to(device)\n",
        "\n",
        "    # Compute Q-values for current states\n",
        "    q = model(states)\n",
        "    # print('q.shape:', q.shape)\n",
        "    q_values = q.gather(1, actions.unsqueeze(1)).squeeze(1)  # Select Q-values of taken actions\n",
        "\n",
        "    # Compute next Q-values from the target network\n",
        "    next_q_values = target_model(next_states).max(1)[0].detach()  # Max Q-value of next state\n",
        "\n",
        "    dones = dones.to(torch.bool)\n",
        "    # Zero next_q_values for terminal states\n",
        "    next_q_values[dones] = 0.0\n",
        "\n",
        "    # Compute target Q-values\n",
        "    target_q_values = rewards + gamma * next_q_values\n",
        "\n",
        "    # Compute loss using Huber loss (smooth_l1_loss)\n",
        "    dq_loss = F.smooth_l1_loss(q_values, target_q_values.detach())\n",
        "\n",
        "    if use_supervised_loss:\n",
        "        l = torch.full_like(q, 0.8)\n",
        "        l[:, actions] = 0\n",
        "        # print('actions', actions)\n",
        "        # print('l', l)\n",
        "        # print('q', q)\n",
        "        # print('q_values', q_values)\n",
        "        # print('q_values.shape:', q_values.shape)\n",
        "        supervised_loss = torch.mean((q + l).max(dim=-1)[0] - q_values)\n",
        "\n",
        "    loss = dq_loss + supervised_loss\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
        "    optimizer.step()\n",
        "    if use_supervised_loss:\n",
        "        return dq_loss.item(), supervised_loss.item()\n",
        "    else:\n",
        "        return loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, demonstrations):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "        self.demonstrations = demonstrations\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((\n",
        "            state,\n",
        "            action,\n",
        "            int(reward),\n",
        "            next_state,\n",
        "            bool(done)\n",
        "        ))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if len(self.buffer) < batch_size // 2:\n",
        "            batch = random.sample(self.demonstrations, batch_size)\n",
        "        else:\n",
        "            batch = random.sample(self.buffer, batch_size // 2)\n",
        "            batch += random.sample(self.demonstrations, batch_size // 2)\n",
        "            random.shuffle(batch)\n",
        "        state, action, reward, next_state, done = zip(*batch)\n",
        "\n",
        "        return (\n",
        "            torch.FloatTensor(np.array(state)) / 255.0,  # Normalize pixels\n",
        "            torch.LongTensor(action),\n",
        "            torch.FloatTensor(reward),\n",
        "            torch.FloatTensor(np.array(next_state)) / 255.0,\n",
        "            torch.FloatTensor(done)\n",
        "        )\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# load expert demonstrations\n",
        "trace = []\n",
        "for i in range(2,3):\n",
        "    with open(f'traces/trace_{i}.pkl', 'rb') as f:\n",
        "        trace += pickle.load(f)\n",
        "\n",
        "def process_frame(frame):\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    frame = cv2.resize(\n",
        "        frame, (84, 84), interpolation=cv2.INTER_AREA\n",
        "    )\n",
        "    return frame\n",
        "\n",
        "demonstrations = []\n",
        "for obj in trace:\n",
        "    demonstrations.append((process_frame(obj['state']), obj['action'], obj['reward'], process_frame(obj['next_state']), obj['done']))\n",
        "\n",
        "demonstrations = [(\n",
        "    np.stack([demonstrations[j][0] for j in range(i-3,i+1)]),\n",
        "    demonstrations[i][1],\n",
        "    demonstrations[i][2],\n",
        "    np.stack([demonstrations[j][3] for j in range(i-3,i+1)]),\n",
        "    demonstrations[i][4])\n",
        "    for i in range(4, len(demonstrations))]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_env():\n",
        "    env = gym.make(\"ALE/Frogger-v5\", render_mode=\"rgb_array\")  # Create Atari env\n",
        "    env = GrayscaleObservation(env, keep_dim=False)\n",
        "    env = ResizeObservation(env, (84, 84))\n",
        "    env = wrap_deepmind(env, episode_life=False, clip_rewards=False, frame_stack=True, scale=False)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def record_video(name_prefix):\n",
        "    env = get_env()\n",
        "    env = RecordVideo(env, video_folder=\"dqfd/videos\", episode_trigger=lambda x: True, name_prefix=name_prefix)\n",
        "    env = RecordEpisodeStatistics(env, buffer_length=1)\n",
        "    state, info = env.reset()\n",
        "    while True:\n",
        "        action = select_action(env, dqn, state, epsilon)\n",
        "        next_state, reward, done, truncated, info = env.step(action)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "    env.close()\n",
        "    return env.return_queue[0], env.length_queue[0], env.time_queue[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1wuMoyn2Lc08",
        "outputId": "37786411-59ff-4c78-f703-249d8d91bd03"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
            "[Powered by Stella]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation space: Box(0, 255, (84, 336), uint8)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `wandb.require('core')` is redundant as it is now the default behavior.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkevinxli\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20250310_195952-w9smf3ua</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kevinxli/frogger/runs/w9smf3ua' target=\"_blank\">dqfd</a></strong> to <a href='https://wandb.ai/kevinxli/frogger' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kevinxli/frogger' target=\"_blank\">https://wandb.ai/kevinxli/frogger</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kevinxli/frogger/runs/w9smf3ua' target=\"_blank\">https://wandb.ai/kevinxli/frogger/runs/w9smf3ua</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kevinxli/frogger/runs/w9smf3ua?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f91bd5050c0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the Atari environment\n",
        "env = get_env()\n",
        "\n",
        "# Check Action / State space\n",
        "obs, info = env.reset()\n",
        "\n",
        "action_dim = env.action_space.n\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dqn = DQN_CNN(4, action_dim).to(device)\n",
        "target_dqn = DQN_CNN(4, action_dim).to(device)\n",
        "target_dqn.load_state_dict(dqn.state_dict())\n",
        "\n",
        "lr = 0.0001\n",
        "weight_decay = 1e-5\n",
        "replay_buffer_size = 10000\n",
        "optimizer = optim.AdamW(dqn.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "replay_buffer = ReplayBuffer(replay_buffer_size, demonstrations)\n",
        "\n",
        "num_pretraining_iterations = 100000\n",
        "num_train_iterations = 1000\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "epsilon = 0.01\n",
        "target_update_freq = 10000\n",
        "rewards_list = []\n",
        "\n",
        "wandb.require(\"core\")\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "      # Set the project where this run will be logged\n",
        "      project=\"frogger\",\n",
        "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "      name=f\"dqfd\",\n",
        "      # Track hyperparameters and run metadata\n",
        "      config={\n",
        "      \"lr\": lr,\n",
        "      \"weight_decay\": weight_decay,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"gamma\": gamma,\n",
        "      \"epsilon\": epsilon,\n",
        "      \"replay_buffer_size\": replay_buffer_size,\n",
        "      \"variant\": \"dqfd\",\n",
        "      \"num_pretraining_iterations\": num_pretraining_iterations,\n",
        "      \"num_train_iterations\": num_train_iterations,\n",
        "      \"target_update_freq\": target_update_freq,\n",
        "      })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.03156 S Loss 0.76866\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_153785/3190481482.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
            "  state = torch.FloatTensor(state).unsqueeze(0) / 255.0  # Normalize pixels\n",
            "  0%|          | 4/100000 [00:10<54:08:45,  1.95s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 4.0, lengths: 1329, time taken: 8.837131\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 998/100000 [00:51<1:09:42, 23.67it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /teamspace/studios/this_studio/dqfd/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.03860 S Loss 0.02591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 1004/100000 [00:59<17:10:05,  1.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 4.0, lengths: 1257, time taken: 7.256957\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1998/100000 [01:43<59:53, 27.28it/s]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.01150 S Loss 0.82132\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 2003/100000 [01:45<5:23:14,  5.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 9.0, lengths: 265, time taken: 1.607035\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 2998/100000 [02:24<1:04:01, 25.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.07003 S Loss 0.02982\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 3004/100000 [02:37<25:27:13,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 3.0, lengths: 1753, time taken: 11.324477\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 4000/100000 [03:17<1:11:29, 22.38it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.02147 S Loss 0.01311\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 4003/100000 [03:20<9:36:00,  2.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 8.0, lengths: 425, time taken: 2.554721\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 5000/100000 [04:04<1:00:30, 26.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.01908 S Loss 0.01625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 5007/100000 [04:08<7:50:25,  3.37it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 9.0, lengths: 625, time taken: 3.503478\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 5998/100000 [04:46<1:16:08, 20.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.01861 S Loss 0.05268\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 6004/100000 [04:49<5:59:49,  4.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 12.0, lengths: 377, time taken: 2.204538\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 6999/100000 [05:23<58:54, 26.31it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 4.30416 S Loss 0.00227\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 7005/100000 [05:27<6:25:52,  4.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 12.0, lengths: 381, time taken: 2.518098\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 7998/100000 [06:06<57:53, 26.49it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 2.23699 S Loss 0.00167\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 8004/100000 [06:09<5:43:42,  4.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 11.0, lengths: 381, time taken: 2.256722\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▉         | 8999/100000 [06:47<54:23, 27.88it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.01358 S Loss 0.00004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▉         | 9005/100000 [06:50<5:35:09,  4.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 8.0, lengths: 457, time taken: 2.232832\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 10000/100000 [07:28<48:01, 31.23it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.01290 S Loss 0.00922\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 10004/100000 [07:31<5:19:43,  4.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 12.0, lengths: 358, time taken: 1.909942\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 10998/100000 [08:08<47:08, 31.47it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 2.02860 S Loss 0.01501\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 11005/100000 [08:11<4:38:54,  5.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 17.0, lengths: 495, time taken: 2.343634\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 11997/100000 [08:41<44:07, 33.24it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.00943 S Loss 0.00579\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 12005/100000 [08:45<4:52:52,  5.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 18.0, lengths: 621, time taken: 2.763642\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 13000/100000 [09:15<40:30, 35.80it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 1.30665 S Loss 0.00777\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 13008/100000 [09:19<5:32:05,  4.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 8.0, lengths: 697, time taken: 3.32441\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 14000/100000 [09:49<41:28, 34.56it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.00378 S Loss 0.00040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 14004/100000 [09:52<6:58:51,  3.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 35.0, lengths: 632, time taken: 3.05099\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▍        | 14999/100000 [10:22<42:54, 33.01it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.00580 S Loss 0.00000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 15003/100000 [10:26<6:30:18,  3.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 16.0, lengths: 541, time taken: 2.843194\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 16000/100000 [10:57<41:46, 33.51it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 1.03982 S Loss 0.00000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 16004/100000 [11:00<5:41:06,  4.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 14.0, lengths: 500, time taken: 2.271989\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 16999/100000 [11:34<50:00, 27.66it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.00407 S Loss 0.00000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 17006/100000 [11:38<6:07:00,  3.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 79.0, lengths: 671, time taken: 3.314191\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 17999/100000 [12:09<45:11, 30.25it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.00804 S Loss 0.00138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 18007/100000 [12:12<3:28:55,  6.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 12.0, lengths: 385, time taken: 1.98006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|█▉        | 18997/100000 [12:43<39:21, 34.30it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 1.40181 S Loss 0.01644\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|█▉        | 19005/100000 [12:46<3:49:03,  5.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 18.0, lengths: 459, time taken: 2.250636\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|█▉        | 19999/100000 [13:16<40:51, 32.63it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.67936 S Loss 0.00000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 20003/100000 [13:20<6:14:49,  3.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 20.0, lengths: 603, time taken: 2.80124\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██        | 20997/100000 [13:50<38:11, 34.48it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.03072 S Loss 0.00000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██        | 21004/100000 [13:53<4:43:46,  4.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 30.0, lengths: 555, time taken: 2.841188\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 21997/100000 [14:23<37:50, 34.36it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.03814 S Loss 0.00039\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 22005/100000 [14:26<3:56:37,  5.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 13.0, lengths: 493, time taken: 2.476319\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 22997/100000 [14:56<43:41, 29.37it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.05883 S Loss 0.02940\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 23005/100000 [14:59<3:54:05,  5.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 20.0, lengths: 541, time taken: 2.443799\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 23997/100000 [15:29<37:09, 34.09it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.02589 S Loss 0.00000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 24005/100000 [15:31<3:00:04,  7.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 12.0, lengths: 357, time taken: 1.847406\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 25000/100000 [16:00<33:52, 36.89it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.00998 S Loss 0.00279\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 25008/100000 [16:03<3:37:43,  5.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 21.0, lengths: 505, time taken: 2.356721\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 25998/100000 [16:38<44:54, 27.46it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.06316 S Loss 0.00000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 26005/100000 [16:41<5:14:44,  3.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 26.0, lengths: 516, time taken: 3.039076\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 26998/100000 [17:16<48:00, 25.34it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.07422 S Loss 0.00000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 27001/100000 [17:20<8:36:44,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 68.0, lengths: 529, time taken: 3.231191\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 27997/100000 [17:58<39:56, 30.05it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.00402 S Loss 0.00000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 28004/100000 [18:02<4:33:31,  4.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode total rewards: 14.0, lengths: 516, time taken: 2.945987\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▉       | 29000/100000 [18:38<47:20, 24.99it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQ Loss 0.00857 S Loss 0.00000\n"
          ]
        }
      ],
      "source": [
        "# Supervised pretraining\n",
        "for iteration in tqdm(range(num_pretraining_iterations)):\n",
        "    dq_loss, s_loss = train(dqn, target_dqn, replay_buffer, optimizer, batch_size, gamma, use_supervised_loss=True)\n",
        "    wandb.log({\"pretrain/loss\": dq_loss + s_loss, \"pretrain/dq_loss\": dq_loss, \"pretrain/supervised_loss\": s_loss})\n",
        "    if iteration % target_update_freq == 0:\n",
        "        target_dqn.load_state_dict(dqn.state_dict())\n",
        "    if iteration % 1000 == 0:\n",
        "        print(f\"DQ Loss {dq_loss:.5f} S Loss {s_loss:.5f}\")\n",
        "        reward, length, time = record_video(name_prefix=f\"pretrain_iter_{iteration}\")\n",
        "        print(f'Episode total rewards: {reward}, lengths: {length}, time taken: {time}')\n",
        "        wandb.log({'pretrain/reward': reward, 'pretrain/length': length, 'pretrain/time': time})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'env' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m state, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      2\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
          ]
        }
      ],
      "source": [
        "state, info = env.reset()\n",
        "total_loss = 0\n",
        "total_reward = 0\n",
        "\n",
        "for iteration in range(num_train_iterations):\n",
        "    action = select_action(env, dqn, state, epsilon)\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "    replay_buffer.push(state, action, reward, next_state, terminated)\n",
        "\n",
        "    if terminated:\n",
        "        state, info = env.reset()\n",
        "        total_loss = 0\n",
        "        total_reward = 0\n",
        "        wandb.log({'train/loss': total_loss, 'train/reward': total_reward})\n",
        "    else:\n",
        "        state = next_state\n",
        "\n",
        "    loss = train(dqn, target_dqn, replay_buffer, optimizer, batch_size, gamma)\n",
        "    total_loss += loss\n",
        "\n",
        "    rewards_list.append(total_reward)\n",
        "\n",
        "    if iteration % target_update_freq == 0:\n",
        "        target_dqn.load_state_dict(dqn.state_dict())\n",
        "        torch.save(dqn.state_dict(), f\"frogger_dqfd_iter_{iteration}.pth\")\n",
        "        reward, length, time = record_video(name_prefix=f\"train_iter_{iteration}\")\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "plt.plot(rewards_list)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"DQN Training Performance on Frogger\")\n",
        "plt.show()\n",
        "\n",
        "# store the model\n",
        "torch.save(dqn.state_dict(), \"frogger_dqfd_model.pth\")\n",
        "# save the rewards_list in a txt file with comma separated\n",
        "np.savetxt(\"frogger_dqn_rewards.txt\", rewards_list, delimiter=\",\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
