{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oydXZWrhKoBY"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import ale_py  # Ensure Atari environments work\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import collections\n",
        "import random\n",
        "from collections import deque\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "from functools import partial\n",
        "import os\n",
        "\n",
        "from utils import get_env, wrap_recording, load_demonstrations, record_video, layer_init, load_llm_demonstrations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-P3Fee-h1bey"
      },
      "outputs": [],
      "source": [
        "class DQN_CNN(nn.Module):\n",
        "    def __init__(self, input_channels, action_dim):\n",
        "        super(DQN_CNN, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),  # Output: (32, 20, 20)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),  # Output: (64, 9, 9)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),  # Output: (64, 7, 7)\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(64*7*7, 512),  # Flattened CNN features\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, action_dim)  # Output Q-values for each action\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x/255.0)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Based on https://github.com/k4ntz/OC_Atari/blob/5386289258c6f240bd107dea0fe41512262281b2/ocatari/utils.py#L122\n",
        "class DQN_MLP(nn.Module):\n",
        "    def __init__(self, input_size, framestack, action_dim):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            layer_init(nn.Linear(input_size, 512)),\n",
        "            nn.ReLU(),\n",
        "            layer_init(nn.Linear(512, 256)),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            layer_init(nn.Linear(256*framestack, 512)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.fc_layer = layer_init(\n",
        "            nn.Linear(512, action_dim), std=0.01)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.network(x/255.0)\n",
        "        x = self.fc_layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xHgk5kkH-hBd"
      },
      "outputs": [],
      "source": [
        "def select_action(env, model, state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()  # Random action (exploration)\n",
        "\n",
        "    state = torch.FloatTensor(np.array(state)).unsqueeze(0)\n",
        "    state = state.to(device)\n",
        "    with torch.no_grad():\n",
        "        return model(state).argmax().item()\n",
        "\n",
        "def train(model, target_model, buffer, optimizer, batch_size, gamma, supervised, use_demo):\n",
        "    if not supervised and buffer.size() < batch_size:\n",
        "        return 0\n",
        "    \n",
        "    # Sample batch from experience replay\n",
        "    states, actions, rewards, next_states, dones = buffer.sample(batch_size, percent_from_demo=1 if supervised else (0.5 if use_demo else 0))\n",
        "\n",
        "    states = states.to(device)\n",
        "    actions = actions.to(device)\n",
        "    rewards = rewards.to(device)\n",
        "    next_states = next_states.to(device)\n",
        "    dones = dones.to(device)\n",
        "\n",
        "    # Compute Q-values for current states\n",
        "    q = model(states)\n",
        "    # print('q.shape:', q.shape)\n",
        "    q_values = q.gather(1, actions.unsqueeze(1)).squeeze(1)  # Select Q-values of taken actions\n",
        "\n",
        "    # Compute next Q-values from the target network\n",
        "    next_q_values = target_model(next_states).max(1)[0].detach()  # Max Q-value of next state\n",
        "\n",
        "    dones = dones.to(torch.bool)\n",
        "    # Zero next_q_values for terminal states\n",
        "    next_q_values[dones] = 0.0\n",
        "\n",
        "    # Compute target Q-values\n",
        "    target_q_values = rewards + gamma * next_q_values\n",
        "\n",
        "    loss = F.mse_loss(q_values, target_q_values.detach())\n",
        "\n",
        "    if supervised:\n",
        "        l = torch.full_like(q, 0.8)\n",
        "        l[:, actions] = 0\n",
        "        supervised_loss = torch.mean((q + l).max(dim=-1)[0] - q_values)\n",
        "        loss += supervised_loss\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
        "    optimizer.step()\n",
        "    return loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, demonstrations):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "        self.demonstrations = demonstrations\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((\n",
        "            state,\n",
        "            action,\n",
        "            int(reward),\n",
        "            next_state,\n",
        "            bool(done)\n",
        "        ))\n",
        "\n",
        "    def sample(self, batch_size, percent_from_demo=0.5):\n",
        "        if len(demonstrations) > 0:\n",
        "            buffer_sample_size = int(batch_size * (1-percent_from_demo))\n",
        "            batch = random.sample(self.buffer, buffer_sample_size)\n",
        "            batch += random.sample(self.demonstrations, batch_size - buffer_sample_size)\n",
        "            random.shuffle(batch)\n",
        "        else:\n",
        "            batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = zip(*batch)\n",
        "\n",
        "        return (\n",
        "            torch.FloatTensor(np.array(state)),\n",
        "            torch.LongTensor(action),\n",
        "            torch.FloatTensor(reward),\n",
        "            torch.FloatTensor(np.array(next_state)),\n",
        "            torch.FloatTensor(done)\n",
        "        )\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1wuMoyn2Lc08",
        "outputId": "37786411-59ff-4c78-f703-249d8d91bd03"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
            "[Powered by Stella]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation space: Box(0, 255, (84, 336), uint8)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `wandb.require('core')` is redundant as it is now the default behavior.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total actions: 280\n",
            "total reward: 35.0\n",
            "total length: 280\n",
            "Loaded 280 demonstration steps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkevinxli\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20250316_204329-ecwqvi2s</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kevinxli/frogger/runs/ecwqvi2s' target=\"_blank\">dqn_demo_sup</a></strong> to <a href='https://wandb.ai/kevinxli/frogger' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kevinxli/frogger' target=\"_blank\">https://wandb.ai/kevinxli/frogger</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kevinxli/frogger/runs/ecwqvi2s' target=\"_blank\">https://wandb.ai/kevinxli/frogger/runs/ecwqvi2s</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kevinxli/frogger/runs/ecwqvi2s?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f1b29539570>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oc = False\n",
        "episodic=True\n",
        "framestack = 4\n",
        "use_demo=True\n",
        "supervised=True\n",
        "project_name = f'dqn{\"_oc\" if oc else \"\"}{\"_demo\" if use_demo else \"\"}{\"_sup\" if supervised else \"\"}'\n",
        "\n",
        "env = get_env(oc=oc, framestack=framestack, episodic=episodic)\n",
        "\n",
        "# Check Action / State space\n",
        "obs, info = env.reset()\n",
        "\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if oc:\n",
        "      dqn = DQN_MLP(input_size=len(env.ns_state), framestack=framestack, action_dim=action_dim).to(device)\n",
        "      target_dqn = DQN_MLP(input_size=len(env.ns_state), framestack=framestack, action_dim=action_dim).to(device)\n",
        "      target_dqn.load_state_dict(dqn.state_dict())\n",
        "else:\n",
        "      dqn = DQN_CNN(4, action_dim).to(device)\n",
        "      target_dqn = DQN_CNN(4, action_dim).to(device)\n",
        "      target_dqn.load_state_dict(dqn.state_dict())\n",
        "\n",
        "lr = 0.0001\n",
        "weight_decay = 1e-5\n",
        "replay_buffer_size = 1000000\n",
        "optimizer = optim.AdamW(dqn.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "if use_demo or supervised:\n",
        "      demonstrations = load_llm_demonstrations(oc=oc)\n",
        "      print(f'Loaded {len(demonstrations)} demonstration steps')\n",
        "replay_buffer = ReplayBuffer(capacity=replay_buffer_size, demonstrations=demonstrations)\n",
        "\n",
        "num_pretraining_iterations = 30000\n",
        "num_train_iterations = 1000000\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "if supervised:\n",
        "      epsilon = 0.01\n",
        "      epsilon_min = 0.01\n",
        "      epsilon_decay = 1\n",
        "else:\n",
        "      epsilon = 1.0\n",
        "      epsilon_min = 0.01\n",
        "      epsilon_decay = 0.999954\n",
        "target_update_freq = 10000\n",
        "rewards_list = []\n",
        "\n",
        "wandb.require(\"core\")\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "      # Set the project where this run will be logged\n",
        "      project=\"frogger\",\n",
        "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "      name=project_name,\n",
        "      # Track hyperparameters and run metadata\n",
        "      config={\n",
        "      \"lr\": lr,\n",
        "      \"weight_decay\": weight_decay,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"gamma\": gamma,\n",
        "      \"epsilon\": epsilon,\n",
        "      \"epsilon_min\": epsilon_min,\n",
        "      \"epsilon_decay\": epsilon_decay,\n",
        "      \"replay_buffer_size\": replay_buffer_size,\n",
        "      \"variant\": project_name,\n",
        "      \"num_train_iterations\": num_train_iterations,\n",
        "      \"target_update_freq\": target_update_freq,\n",
        "      })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/30001 [00:00<?, ?it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /teamspace/studios/this_studio/dqn_demo_sup/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            " 33%|███▎      | 9986/30001 [01:27<02:13, 149.86it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /teamspace/studios/this_studio/dqn_demo_sup/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "100%|██████████| 30001/30001 [04:03<00:00, 123.06it/s]\n"
          ]
        }
      ],
      "source": [
        "if supervised:\n",
        "    os.makedirs(f'{project_name}/pretrained', exist_ok=True)\n",
        "\n",
        "    for iteration in tqdm(range(num_pretraining_iterations+1)):\n",
        "        loss = train(dqn, target_dqn, replay_buffer, optimizer, batch_size, gamma, supervised=True, use_demo=use_demo)\n",
        "        wandb.log({\"pretrain/loss\": loss})\n",
        "        if iteration % target_update_freq == 0:\n",
        "            target_dqn.load_state_dict(dqn.state_dict())\n",
        "            reward, length, time = record_video(env=get_env(oc=oc, framestack=framestack), select_action=partial(select_action, model=dqn, epsilon=0),\n",
        "                        video_folder=f\"{project_name}/videos\", episode_trigger=lambda x: True, name_prefix=f\"pretrain_iter_{iteration}\")\n",
        "            wandb.log({'pretrain/reward': reward, 'pretrain/length': length, 'pretrain/time': time})\n",
        "            torch.save(dqn.state_dict(), f'{project_name}/pretrained/{iteration}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 434857/1000001 [1:51:01<8:20:21, 18.82it/s] "
          ]
        }
      ],
      "source": [
        "os.makedirs(f'{project_name}/train', exist_ok=True)\n",
        "\n",
        "optimizer = optim.AdamW(dqn.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "state, info = env.reset()\n",
        "total_loss = 0\n",
        "total_reward = 0\n",
        "\n",
        "for iteration in tqdm(range(num_train_iterations+1)):\n",
        "    action = select_action(env, dqn, state, epsilon)\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "    done = terminated or truncated\n",
        "\n",
        "    replay_buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "    if done:\n",
        "        state, info = env.reset()\n",
        "        wandb.log({'train/episode_loss': total_loss})\n",
        "        total_loss = 0\n",
        "        total_reward = 0\n",
        "    else:\n",
        "        state = next_state\n",
        "\n",
        "    loss = train(dqn, target_dqn, replay_buffer, optimizer, batch_size, gamma, supervised=False, use_demo=use_demo)\n",
        "    wandb.log({'train/loss': loss, 'train/epsilon': epsilon})\n",
        "    total_loss += loss\n",
        "\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    rewards_list.append(total_reward)\n",
        "\n",
        "    if iteration % target_update_freq == 0:\n",
        "        target_dqn.load_state_dict(dqn.state_dict())\n",
        "        torch.save(dqn.state_dict(), f\"{project_name}/train/frogger_iter_{iteration}.pth\")\n",
        "        reward, length, time = record_video(env=get_env(oc=oc, framestack=framestack), select_action=partial(select_action, model=dqn, epsilon=0), video_folder=f\"{project_name}/videos\", episode_trigger=lambda x: True, name_prefix=f\"train_iter_{iteration}\")\n",
        "        wandb.log({'train/reward': reward, 'train/length': length, 'train/time': time})\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
