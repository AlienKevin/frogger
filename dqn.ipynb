{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oydXZWrhKoBY"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import ale_py  # Ensure Atari environments work\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import collections\n",
        "import random\n",
        "from collections import deque\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "from functools import partial\n",
        "import os\n",
        "\n",
        "from utils import get_env, wrap_recording, load_demonstrations, record_video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-P3Fee-h1bey"
      },
      "outputs": [],
      "source": [
        "class DQN_CNN(nn.Module):\n",
        "    def __init__(self, input_channels, action_dim):\n",
        "        super(DQN_CNN, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),  # Output: (32, 20, 20)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),  # Output: (64, 9, 9)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),  # Output: (64, 7, 7)\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(64*7*7, 512),  # Flattened CNN features\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, action_dim)  # Output Q-values for each action\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xHgk5kkH-hBd"
      },
      "outputs": [],
      "source": [
        "def select_action(env, model, state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()  # Random action (exploration)\n",
        "\n",
        "    state = torch.FloatTensor(np.array(state)).unsqueeze(0) / 255.0  # Normalize pixels\n",
        "    state = state.to(device)\n",
        "    with torch.no_grad():\n",
        "        return model(state).argmax().item()\n",
        "\n",
        "def train(model, target_model, buffer, optimizer, batch_size, gamma):\n",
        "    if buffer.size() < batch_size:\n",
        "        return 0\n",
        "    \n",
        "    # Sample batch from experience replay\n",
        "    states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "\n",
        "    states = states.to(device)\n",
        "    actions = actions.to(device)\n",
        "    rewards = rewards.to(device)\n",
        "    next_states = next_states.to(device)\n",
        "    dones = dones.to(device)\n",
        "\n",
        "    # Compute Q-values for current states\n",
        "    q = model(states)\n",
        "    # print('q.shape:', q.shape)\n",
        "    q_values = q.gather(1, actions.unsqueeze(1)).squeeze(1)  # Select Q-values of taken actions\n",
        "\n",
        "    # Compute next Q-values from the target network\n",
        "    next_q_values = target_model(next_states).max(1)[0].detach()  # Max Q-value of next state\n",
        "\n",
        "    dones = dones.to(torch.bool)\n",
        "    # Zero next_q_values for terminal states\n",
        "    next_q_values[dones] = 0.0\n",
        "\n",
        "    # Compute target Q-values\n",
        "    scaled_rewards = 0.01 * rewards\n",
        "    target_q_values = scaled_rewards + gamma * next_q_values\n",
        "\n",
        "    dq_loss = F.mse_loss(q_values, target_q_values.detach())\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    dq_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
        "    optimizer.step()\n",
        "    return dq_loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((\n",
        "            state,\n",
        "            action,\n",
        "            int(reward),\n",
        "            next_state,\n",
        "            bool(done)\n",
        "        ))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = zip(*batch)\n",
        "\n",
        "        return (\n",
        "            torch.FloatTensor(np.array(state)) / 255.0,  # Normalize pixels\n",
        "            torch.LongTensor(action),\n",
        "            torch.FloatTensor(reward),\n",
        "            torch.FloatTensor(np.array(next_state)) / 255.0,\n",
        "            torch.FloatTensor(done)\n",
        "        )\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1wuMoyn2Lc08",
        "outputId": "37786411-59ff-4c78-f703-249d8d91bd03"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
            "[Powered by Stella]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation space: Box(0, 255, (84, 336), uint8)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `wandb.require('core')` is redundant as it is now the default behavior.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkevinxli\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20250311_064429-s2av8cbv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kevinxli/frogger/runs/s2av8cbv' target=\"_blank\">dqn</a></strong> to <a href='https://wandb.ai/kevinxli/frogger' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kevinxli/frogger' target=\"_blank\">https://wandb.ai/kevinxli/frogger</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kevinxli/frogger/runs/s2av8cbv' target=\"_blank\">https://wandb.ai/kevinxli/frogger/runs/s2av8cbv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kevinxli/frogger/runs/s2av8cbv?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f7be16d0850>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the Atari environment\n",
        "env = get_env()\n",
        "\n",
        "# Check Action / State space\n",
        "obs, info = env.reset()\n",
        "\n",
        "action_dim = env.action_space.n\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dqn = DQN_CNN(4, action_dim).to(device)\n",
        "target_dqn = DQN_CNN(4, action_dim).to(device)\n",
        "target_dqn.load_state_dict(dqn.state_dict())\n",
        "\n",
        "lr = 0.0001\n",
        "weight_decay = 1e-5\n",
        "replay_buffer_size = 10000\n",
        "optimizer = optim.AdamW(dqn.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "replay_buffer = ReplayBuffer(replay_buffer_size)\n",
        "\n",
        "num_train_iterations = 1000000\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "epsilon = 0.01\n",
        "target_update_freq = 10000\n",
        "rewards_list = []\n",
        "\n",
        "wandb.require(\"core\")\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "      # Set the project where this run will be logged\n",
        "      project=\"frogger\",\n",
        "      # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
        "      name=f\"dqn\",\n",
        "      # Track hyperparameters and run metadata\n",
        "      config={\n",
        "      \"lr\": lr,\n",
        "      \"weight_decay\": weight_decay,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"gamma\": gamma,\n",
        "      \"epsilon\": epsilon,\n",
        "      \"replay_buffer_size\": replay_buffer_size,\n",
        "      \"variant\": \"dqn\",\n",
        "      \"num_train_iterations\": num_train_iterations,\n",
        "      \"target_update_freq\": target_update_freq,\n",
        "      })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /teamspace/studios/this_studio/dqn/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "os.makedirs('dqn/train', exist_ok=True)\n",
        "\n",
        "optimizer = optim.AdamW(dqn.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "state, info = env.reset()\n",
        "total_loss = 0\n",
        "total_reward = 0\n",
        "\n",
        "for iteration in range(num_train_iterations+1):\n",
        "    action = select_action(env, dqn, state, epsilon)\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "    replay_buffer.push(state, action, reward, next_state, terminated)\n",
        "\n",
        "    if terminated:\n",
        "        state, info = env.reset()\n",
        "        wandb.log({'train/loss': total_loss})\n",
        "        total_loss = 0\n",
        "        total_reward = 0\n",
        "    else:\n",
        "        state = next_state\n",
        "\n",
        "    loss = train(dqn, target_dqn, replay_buffer, optimizer, batch_size, gamma)\n",
        "    total_loss += loss\n",
        "\n",
        "    rewards_list.append(total_reward)\n",
        "\n",
        "    if iteration % target_update_freq == 0:\n",
        "        target_dqn.load_state_dict(dqn.state_dict())\n",
        "        torch.save(dqn.state_dict(), f\"dqn/train/frogger_dqn_iter_{iteration}.pth\")\n",
        "        reward, length, time = record_video(select_action=partial(select_action, model=dqn, epsilon=0), video_folder=\"dqn/videos\", episode_trigger=lambda x: True, name_prefix=f\"train_iter_{iteration}\")\n",
        "        wandb.log({'train/reward': reward, 'train/length': length, 'train/time': time})\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
